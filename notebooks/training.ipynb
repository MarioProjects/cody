{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import wandb\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "from transformers import get_scheduler\n",
    "\n",
    "from peft import LoraConfig, TaskType\n",
    "from peft import get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utils from ../src/utils\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.datasets import get_code_alpaca_20k\n",
    "from utils.completions import clean_completion, inference\n",
    "from utils.evaluation import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Constants: Model, Tokenizer & Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The difference between “it” aka “Instruction Tuned”\n",
    "and the base model is that the “it” variants are better for chat purposes\n",
    "since they have been fine-tuned to better understand the instructions\n",
    "and generate better answers while the base variants are those that have not undergone\n",
    "under any sort of fine-tuning. They can still generate answers but not as good as the “it” one.\n",
    "\n",
    "\"\"\"\n",
    "# google/gemma-2b | google/gemma-2b-it | microsoft/phi-2\n",
    "# Qwen/Qwen1.5-0.5B | Qwen/Qwen1.5-0.5B-Chat\n",
    "model_name = \"Qwen/Qwen1.5-0.5B-Chat\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: Qwen/Qwen1.5-0.5B-Chat\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\":0},\n",
    ")\n",
    "print(f\"Model loaded: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\", \"dense\"],\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 15,138,816 || all params: 479,126,528 || trainable%: 3.159669756378006\n"
     ]
    }
   ],
   "source": [
    "lora_model = get_peft_model(model, lora_config)\n",
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded: Qwen/Qwen1.5-0.5B-Chat\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=lora_model)\n",
    "max_length = 1024\n",
    "print(f\"Tokenizer loaded: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    # Tokenize the codes\n",
    "    tokenized_outputs = tokenizer(examples[\"code\"], truncation=True, max_length=max_length) # , padding='max_length'\n",
    "    \n",
    "    # Set labels to input_ids. This assumes a task like text generation where\n",
    "    # the model learns to predict the input sequence itself (next word).\n",
    "    # You don’t need labels (also known as an unsupervised task)\n",
    "    # because the next word is the label\n",
    "    tokenized_outputs[\"labels\"] = tokenized_outputs[\"input_ids\"].copy()\n",
    "    return tokenized_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: Code Alpaca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1  # number of examples in each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fbde44dcd9543758f9dfcad4f5c5fff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3259 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_dataset = get_code_alpaca_20k()\n",
    "tokenized_dataset = base_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.with_format(\"torch\")\n",
    "\n",
    "split_dataset = tokenized_dataset[\"train\"].train_test_split(\n",
    "    test_size=0.1, shuffle=True, seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch dataloader format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['output', 'input', 'instruction', 'code', 'prompt', 'completion', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2933\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['output', 'input', 'instruction', 'code', 'prompt', 'completion', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 326\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns that can't be converted to tensors\n",
    "dataset = split_dataset.remove_columns(['output', 'input', 'instruction', 'code', 'prompt', 'completion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the data to tensors\n",
    "dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 2933\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    dataset[\"train\"],  # For testing purposes ->  .shuffle(seed=42).select(range(1000)),\n",
    "    shuffle=True, batch_size=batch_size,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "print(f\"Training dataset size: {len(dataset['train'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing dataset size: 326\n"
     ]
    }
   ],
   "source": [
    "test_dataloader = DataLoader(\n",
    "    dataset[\"test\"],  # For testing purposes -> .shuffle(seed=42).select(range(1000)),\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "print(f\"Testing dataset size: {len(dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check - Generate Batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': torch.Size([1, 129]), 'attention_mask': torch.Size([1, 129]), 'labels': torch.Size([1, 129])}\n"
     ]
    }
   ],
   "source": [
    "print({k: v.shape for k, v in batch.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try the base model (not finetuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      "def count_characters(text, char):\n",
      "    \"\"\" Write a function to count the number of times a given character is found in an\n",
      "    input string.\n",
      "    \"\"\"\n",
      "\n",
      "COMPLETION:\n",
      "    count = 0\n",
      "    for c in text:\n",
      "        if c == char:\n",
      "            count += 1\n",
      "    return count\n",
      "\n"
     ]
    }
   ],
   "source": [
    "case = 2\n",
    "full_text = split_dataset['test'][case]['code']\n",
    "prompt_text = split_dataset['test'][case]['prompt']\n",
    "completion_text = split_dataset['test'][case]['completion']\n",
    "print(f\"PROMPT:\\n{prompt_text}\")\n",
    "print(f\"\\nCOMPLETION:\\n{completion_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def count_characters(text, char):\\n    \"\"\" Write a function to count the number of times a given character is found in an\\n    input string.\\n    \"\"\"'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model's answer: \")\n",
    "response = inference(prompt_text, lora_model, tokenizer, max_output_tokens=256)\n",
    "clean_response = clean_completion(response, tokenizer.eos_token, prompt_text)\n",
    "print(clean_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 2e-5\n",
    "weight_decay = 0.01\n",
    "\n",
    "num_epochs = 0.1\n",
    "\n",
    "# The desired batch size is the batch size you want to train with\n",
    "desired_batch_size = 8\n",
    "gradient_accumulation_steps = desired_batch_size // batch_size\n",
    "\n",
    "# We set the maximum number of iterations to those ones needed to go through the dataset\n",
    "# num_epochs times, considering the gradient accumulation\n",
    "max_iters = int(num_epochs * len(train_dataloader) // gradient_accumulation_steps)\n",
    "eval_interval = 25\n",
    "\n",
    "print(f\"Max iterations: {max_iters}\")\n",
    "print(f\"Evaluation interval: {eval_interval}\")\n",
    "      \n",
    "warmup_steps_ratio = 0.1\n",
    "warmup_steps = math.ceil(max_iters * warmup_steps_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "wandb_project = 'cody'\n",
    "run = wandb.init(project=wandb_project, config={\n",
    "    \"model\": model_name,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"weight_decay\": weight_decay,\n",
    "    \"num_epochs\": num_epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"max_length\": max_length,\n",
    "    \"warmup_steps\": warmup_steps,\n",
    "    \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
    "    \"max_iters\": max_iters,\n",
    "    \"eval_interval\": eval_interval,\n",
    "    \"prompt\": prompt_text,\n",
    "    \"completion\": completion_text,\n",
    "    \"case\": case\n",
    "})\n",
    "print(f'Run name: {run.name}. Visit at {run.get_url()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer and learning rate scheduler\n",
    "\n",
    "Create an optimizer and learning rate scheduler to fine-tune the model. Let’s use the [AdamW](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html) optimizer from PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(lora_model.parameters(), lr=learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the default learning rate scheduler from [Trainer](https://huggingface.co/docs/transformers/v4.36.1/en/main_classes/trainer#transformers.Trainer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"cosine\", optimizer=optimizer,\n",
    "    num_warmup_steps=warmup_steps, num_training_steps=max_iters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through epochs accross the whole dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "metrics = {\"train\": {\"loss\": [], \"perplexity\": []}, \"test\": {\"loss\": [], \"perplexity\": []}}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    lora_model.train()\n",
    "    for batch_idx, batch in enumerate(train_dataloader):\n",
    "        outputs = lora_model(**batch)\n",
    "        # El modelo calcula su loss, pero podriamos acceder a los logits del modelo\n",
    "        # y las labels del batch y calcular nuestra loss propia\n",
    "        # scale the loss to account for gradient accumulation\n",
    "        loss = outputs.loss / gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "\n",
    "        if batch_idx % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        progress_bar.update(1)\n",
    "\n",
    "    train_loss, train_perplexity = evaluate(lora_model, train_dataloader)\n",
    "    test_loss, test_perplexity = evaluate(lora_model, test_dataloader)\n",
    "    metrics[\"train\"][\"loss\"].append(train_loss)\n",
    "    metrics[\"train\"][\"perplexity\"].append(train_perplexity)\n",
    "    metrics[\"test\"][\"loss\"].append(test_loss)\n",
    "    metrics[\"test\"][\"perplexity\"].append(test_perplexity)\n",
    "\n",
    "    print(f\"### EPOCH {epoch+1} ###\")\n",
    "    print(f\"Train Loss: {train_loss:.4f} - Train Perplexity: {train_perplexity:.4f}\")\n",
    "    print(f\"Test Loss: {test_loss:.4f} - Test Perplexity: {test_perplexity:.4f}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating over the dataset in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress_bar = tqdm(range(max_iters))\n",
    "metrics = {\"train\": {\"loss\": [], \"perplexity\": []}, \"test\": {\"loss\": [], \"perplexity\": []}}\n",
    "\n",
    "for iter_num in range(max_iters):\n",
    "    model.train()\n",
    "    \n",
    "    for micro_step in range(gradient_accumulation_steps):\n",
    "        # Extract a batch of data\n",
    "        batch = next(iter(train_dataloader))\n",
    "\n",
    "        outputs = lora_model(**batch)\n",
    "        # El modelo calcula su loss, pero podriamos acceder a los logits del modelo\n",
    "        # y las labels del batch y calcular nuestra loss propia\n",
    "        # scale the loss to account for gradient accumulation\n",
    "        loss = outputs.loss / gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "    progress_bar.update(1)\n",
    "\n",
    "    if iter_num % eval_interval == 0:\n",
    "\n",
    "        # scale up to undo the division above\n",
    "        # approximating total loss (exact would have been a sum)\n",
    "        train_loss = loss.item() * gradient_accumulation_steps\n",
    "\n",
    "        test_loss, test_perplexity = evaluate(model, test_dataloader)\n",
    "        #train_loss, train_perplexity = evaluate(model, train_dataloader)\n",
    "        #metrics[\"train\"][\"loss\"].append(train_loss)\n",
    "        #metrics[\"train\"][\"perplexity\"].append(train_perplexity)\n",
    "        metrics[\"test\"][\"loss\"].append(test_loss)\n",
    "        metrics[\"test\"][\"perplexity\"].append(test_perplexity)\n",
    "\n",
    "        print(f\"### ITER {iter_num} ###\")\n",
    "        #print(f\"Train Loss: {train_loss:.4f} - Train Perplexity: {train_perplexity:.4f}\")\n",
    "        print(f\"Test Loss: {test_loss:.4f} - Test Perplexity: {test_perplexity:.4f}\")\n",
    "\n",
    "        wandb.log({\n",
    "            \"iter\": iter_num,\n",
    "            \"train/loss\": train_loss,\n",
    "            \"val/loss\": test_loss,\n",
    "            \"val/perplexity\": test_perplexity,\n",
    "            \"lr\": lr_scheduler.get_last_lr()[0],\n",
    "        })\n",
    "\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.save_pretrained(f\"checkpoints/{run.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = 25\n",
    "assert test_samples <= len(dataset['test']), \"Not enough samples for testing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_table = wandb.Table(columns=[\"Prompt\", \"Completion\", \"Model Completion\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for case in tqdm(range(test_samples)):\n",
    "    full_text = split_dataset['test'][case]['code']\n",
    "    prompt_text = split_dataset['test'][case]['prompt']\n",
    "    completion_text = split_dataset['test'][case]['completion']\n",
    "\n",
    "    response = inference(prompt_text, lora_model, tokenizer, max_output_tokens=256)\n",
    "    clean_response = clean_completion(response, tokenizer.eos_token, prompt_text)\n",
    "    \n",
    "    test_table.add_data(prompt_text, completion_text, clean_response)\n",
    "    \n",
    "    \"\"\"\n",
    "    print(f\"PROMPT:\\n{prompt_text}\")\n",
    "    print(f\"\\nCOMPLETION:\\n{completion_text}\")\n",
    "    print(\"Model's answer: \")\n",
    "    print(clean_response)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log the table to wandb\n",
    "run.log({\"test_completions\": test_table})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HumanEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from human_eval.data import write_jsonl, read_problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems = read_problems()\n",
    "print(f\"Loaded {len(problems)} problems\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can generate more than one candidate per task\n",
    "# later one pass@1, pass@10... will be used to evaluate the model\n",
    "num_samples_per_task = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results= []\n",
    "for task_id in tqdm(problems):\n",
    "    for _ in range(num_samples_per_task):\n",
    "        response = inference(problems[task_id]['prompt'], lora_model, tokenizer, max_output_tokens=256)\n",
    "        clean_response = clean_completion(response, tokenizer.eos_token, prompt_text)\n",
    "\n",
    "        results.append({\n",
    "            'task_id': task_id,\n",
    "            'completion': clean_response,\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the results under (f\"checkpoints/{run.name}\")\n",
    "write_jsonl(f\"checkpoints/{run.name}/human_eval-{num_samples_per_task}_results.jsonl\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model and the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact = wandb.Artifact(\"checkpoint_and_results\", type=\"models\")\n",
    "artifact.add_dir(f\"checkpoints/{run.name}\")\n",
    "run.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
