{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from tqdm.auto import tqdm\n",
    "from human_eval.data import write_jsonl, read_problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_MODEL_NAME = \"deepseek-coder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 164 problems\n"
     ]
    }
   ],
   "source": [
    "problems = read_problems()\n",
    "print(f\"Loaded {len(problems)} problems\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can generate more than one candidate per task\n",
    "# later one pass@1, pass@10... will be used to evaluate the model\n",
    "num_samples_per_task = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "def generate_foo():\n",
      "    \"\"\"\n",
      "    Return the string foo.\n",
      "\n",
      "    For example:\n",
      "    foo() => foo\n",
      "    \"\"\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "foo_task = '\\ndef generate_foo():\\n    \"\"\"\\n    Return the string foo.\\n\\n    For example:\\n    foo() => foo\\n    \"\"\"\\n'\n",
    "print(foo_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_one_completion(prompt):\n",
    "    response = ollama.chat(\n",
    "        model=LLM_MODEL_NAME,\n",
    "        messages=[\n",
    "            {\n",
    "                'role': 'system',\n",
    "                'content': 'You are a software developer. You need to just write how the code continues.'\n",
    "            },\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': f\"Continue the code generation for the followind function:\\n{foo_task}\"\n",
    "            },\n",
    "            {\n",
    "                'role': 'assistant',\n",
    "                'content': \"return 'foo'\"\n",
    "            },\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': f\"Continue the code generation for the followind function:\\n{prompt}\"\n",
    "            },\n",
    "        ],\n",
    "        options={'temperature': 0.25}\n",
    "    )\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a subset of 5 problems to test the model, problems is a dict\n",
    "#problems = {k: problems[k] for k in list(problems.keys())[:1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d80664b8341e42a2b2ed334fd85e5662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results= []\n",
    "for task_id in tqdm(problems):\n",
    "    for _ in range(num_samples_per_task):\n",
    "        results.append({\n",
    "            'task_id': task_id,\n",
    "            'completion': generate_one_completion(problems[task_id]['prompt']),\n",
    "        })\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from typing import List\n",
      "import math\n",
      "def has_close_elements(numbers:List[float], threshold: float)->bool :   # Define the function with correct type hinting and docstring \n",
      "    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\n",
      "        Given Threshold. Return True when there is a pair that's within 'threshold', else False.\"\"\"    \n",
      "        \n",
      "    for ix1 , num_a in enumerate(numbers):  # Iterate over the elements and their indices  \n",
      "       for ix2, num_b in enumerate(numbers[ix1+1:]):      # Start from next index after current element to avoid duplicate comparisons.   \n",
      "           if math.isclose (abs(num_a - num_b),threshold ,rel_tol=0.5  ) :   # Use is close function for floating point comparison, with tolerance set at half of the threshold value    \n",
      "               return True      # Return true as soon found a pair that's within 'Threshold'.   \n",
      "       if ix1 == len(numbers)-2:        # If it reached last element in list then break out from loop.  \n",
      "           continue \n",
      "            \n",
      "    return False         # No pairs were closer than the threshold, so default to returning false     .      \"\"\"          This function will not work correctly for large lists as its time complexity is O(n^2) which can be slow on larger inputs.\"\"\"       \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(results[0][\"completion\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the rest of the problems with 'return None'\n",
    "for task_id in problems:\n",
    "    if task_id in results:\n",
    "        continue\n",
    "    for _ in range(num_samples_per_task):\n",
    "        results.append({\n",
    "            'task_id': task_id,\n",
    "            'completion': 'return None',\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_jsonl(f\"submission_ollama_{LLM_MODEL_NAME}.jsonl\", results)\n",
    "# Now run: $ evaluate_functional_correctness samples.jsonl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
