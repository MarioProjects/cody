{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phi-1\n",
    "https://huggingface.co/microsoft/phi-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Models\n",
    "microsoft/phi-1 (1.3 billion parameters)\n",
    "microsoft/phi-1_5 (1.3 billion parameters)\n",
    "microsoft/phi-2 (2.7 billion parameters)\n",
    "Qwen/Qwen1.5-0.5B-Chat\n",
    "\"\"\"\n",
    "LLM_MODEL_NAME = \"Qwen/Qwen1.5-0.5B-Chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3ede32897c1418eb5dd6958af7fab9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b898e60c7fcb4c59ae7cdd8a05a66441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/206 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eaa06669a754b57b67d1c2e9969a969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01cfd4caee1d471a86998e62497e262d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b34999ee0d2c4cb1b2b6c5ec51843912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dddb2a83de5a4f40928fe40b818c2777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLM_MODEL_NAME,\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    LLM_MODEL_NAME,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(\"Model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_statement = '''\n",
    "def double_number(num):\n",
    "    \"\"\"\n",
    "    Returns the double of the input number.\n",
    "\n",
    "    Parameters:\n",
    "    num (int or float): The number to be doubled.\n",
    "\n",
    "    Returns:\n",
    "    int or float: The double of the input number.\n",
    "    \"\"\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "   problem_statement,\n",
    "   return_tensors=\"pt\",\n",
    "   return_attention_mask=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(**inputs, max_length=256, temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "def double_number(num):\n",
      "    \"\"\"\n",
      "    Returns the double of the input number.\n",
      "\n",
      "    Parameters:\n",
      "    num (int or float): The number to be doubled.\n",
      "\n",
      "    Returns:\n",
      "    int or float: The double of the input number.\n",
      "    \"\"\"\n",
      "    return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return, return return return return, return return,\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.batch_decode(outputs)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return, return return return return, return return\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = tokenizer.batch_decode(outputs)[0]\n",
    "# cut the text at tokenizer.eos_token\n",
    "text = text[: text.find(tokenizer.eos_token)]\n",
    "# remove the problem statement\n",
    "text = text[len(problem_statement):]\n",
    "# the text should have indentation as is part of a function\n",
    "# if in a new line it not indented, remove it till the end\n",
    "res = \"\"\n",
    "for line in text.split(\"\\n\"):\n",
    "    # if the line is not empty, and not indented, break\n",
    "    if line and not line.startswith(\" \"):\n",
    "        break\n",
    "    res += line + \"\\n\"\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from human_eval.data import write_jsonl, read_problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 164 problems\n"
     ]
    }
   ],
   "source": [
    "problems = read_problems()\n",
    "print(f\"Loaded {len(problems)} problems\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_one_completion(problem_statement, temp=0.0):\n",
    "    inputs = tokenizer(\n",
    "        problem_statement,\n",
    "        return_tensors=\"pt\",\n",
    "        return_attention_mask=False\n",
    "    )\n",
    "    outputs = model.generate(**inputs, max_length=512, temperature=temp)\n",
    "    text = tokenizer.batch_decode(outputs)[0]\n",
    "    # cut the text at tokenizer.eos_token\n",
    "    text = text[: text.find(tokenizer.eos_token)]\n",
    "    # remove the problem statement\n",
    "    text = text[len(problem_statement):]\n",
    "\n",
    "    # the text should have indentation as is part of a function\n",
    "    # if in a new line it not indented, remove it till the end\n",
    "    res = \"\"\n",
    "    for line in text.split(\"\\n\"):\n",
    "        # if the line is not empty, and not indented, break\n",
    "        if line and not line.startswith(\" \"):\n",
    "            break\n",
    "        res += line + \"\\n\"\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can generate more than one candidate per task\n",
    "# later one pass@1, pass@10... will be used to evaluate the model\n",
    "num_samples_per_task = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a83776fe2e240eaa96192e08e5017b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results= []\n",
    "for task_id in tqdm(problems):\n",
    "    for _ in range(num_samples_per_task):\n",
    "        results.append({\n",
    "            'task_id': task_id,\n",
    "            'completion': generate_one_completion(\n",
    "                problems[task_id]['prompt'], temp=0.1\n",
    "            ),\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_jsonl(f\"submission_hf_{LLM_MODEL_NAME.replace('/', '-')}.jsonl\", results)\n",
    "# Now run: $ evaluate_functional_correctness samples.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maparla/anaconda3/envs/mlenv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "h15 = generate_one_completion(problems[\"HumanEval/15\"][\"prompt\"], temp=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    return''.join(str(i) for i in range(n+1))\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(h15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
